<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mapping]]></title>
    <url>%2F2018%2F08%2F28%2Fmapping%2F</url>
    <content type="text"><![CDATA[https://www.elastic.co/blog/logstash_lesson_elasticsearch_mapping 12345678910111213141516171819POST imore-2018.07.25/doc/_search&#123; &quot;query&quot;:&#123;&quot;match_all&quot;: &#123;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;e_terms&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;e.keyword&quot;&#125;, &quot;aggs&quot;: &#123; &quot;f_terms&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;f.keyword&quot;&#125;, &quot;aggs&quot;: &#123; &quot;g_terms&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;g.keyword&quot;&#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>ElasticSsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cluster_block_exception]]></title>
    <url>%2F2018%2F08%2F26%2Fcluster-block-exception%2F</url>
    <content type="text"><![CDATA[在导入数据的时候遇见了一个错误1retrying failed action with response code: 403 (&#123;&quot;type&quot;=&gt;&quot;cluster_block_exception&quot;, &quot;reason&quot;=&gt;&quot;blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];&quot;&#125;) 这是因为 read_only_allow_delete: false 被设置为 true 了, 只要把它设置回 false 就会继续导入数据.但是, 需要去查看设置为 true 的原因. 因为在硬盘占用达到 95% 的时候会自动设置为 true.这个时候如果设置回 false 过一段时间, 它也会设值为 true. 需要用户清除数据或者扩大硬盘大小才能继续.]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硬盘使用情况查看]]></title>
    <url>%2F2018%2F07%2F26%2F%E7%A1%AC%E7%9B%98%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[df命令是linux系统以磁盘分区为单位查看文件系统，可以加上参数查看磁盘剩余空间信息，命令格式：df -hl显示格式为：12345678910文件系统 容量 已用 可用 已用% 挂载点Filesystem Size Used Avail Use% Mounted on/dev/hda2 45G 19G 24G 44% //dev/hda1 494M 19M 450M 4% /boot/dev/hda6 4.9G 2.2G 2.5G 47% /home/dev/hda5 9.7G 2.9G 6.4G 31% /optnone 1009M 0 1009M 0% /dev/shm/dev/hda3 9.7G 7.2G 2.1G 78% /usr/local/dev/hdb2 75G 75G 0 100% //dev/hdb2 75G 75G 0 100% / 以上面的输出为例，表示的意思为： HD硬盘接口的第二个硬盘（b），第二个分区（2），容量是75G，用了75G，可用是0，因此利用率是100%， 被挂载到根分区目录上（/）。 下面是相关命令的解释：1234df -hl 查看磁盘剩余空间df -h 查看每个根路径的分区大小du -sh [目录名] 返回该目录的大小du -sm [文件夹] 返回该文件夹总M数 更多功能可以输入一下命令查看：12df --helpdu --help 查看linux文件目录的大小和文件夹包含的文件数 统计总数大小12345du -sh xmldb/du -sm * | sort -n //统计当前目录大小 并安大小 排序du -sk * | sort -ndu -sk * | grep guojf //看一个人的大小du -m | cut -d &quot;/&quot; -f 2 //看第二个/ 字符前的文字 查看此文件夹有多少文件 ///* 有多少文件123du xmldb/du xmldb/*/*/* |wc -l40752 解释：wc [-lmw]参数说明：-l :多少行；-m:多少字符；-w:多少字两个命令df 、du结合比较直观123df -h 查看整台服务器的硬盘使用情况cd / 进入根目录du -sh * 查看每个文件夹的大小 这样的组合可以快速定位大文件和分区满了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK: ELK 配置]]></title>
    <url>%2F2018%2F07%2F24%2FELK-ELK-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Elasticsearch 配置Elasticsearch 配置文件在 elasticsearch/config 文件夹下. 这个文件夹有两个配置文件 elasticsearch.yml 是配置不同模块的配置文件, logging.yml 是配置日志的配置文件. docker 映射配置文件12345678910111213141516/opt/kibana/config/kibana.yml # kibana 的配置文件/opt/logstash/config/logstash.yml # logstash 的配置文件/opt/elasticsearch/config/elasticsearch.yml # es 的配置文件/usr/share/elasticsearch/data # es 存储数据的位置 docker run -p 8001:5601 -p 8002:9200 -p 8003:9300 -p 8004:5044 \ -v /data/elk/logstash/conf.d:/etc/logstash/conf.d \ -v /data/elk/kibana/config/kibana.yml:/opt/kibana/config/kibana.yml \ -v /data/elk/logstash/config/logstash.yml:/opt/logstash/config/logstash.yml \ -v /data/elk/elasticsearch/config/elasticsearch.yml:/opt/elasticsearch/config/elasticsearch.yml \ -v /data/elk/elasticsearch/data:/usr/share/elasticsearch/data \ -v /data/elk/elasticsearch/log/:/var/lib/elasticsearch \ -v /data/elk/kibana/run_log/:/var/log/kibana/ \ -v /data/elk/logstash/run_log/:/var/log/logstash/ \ -v /data/elk/elasticsearch/run_log/:/var/log/elasticsearch/ \ -it --name elk sebp/elk LogStash 配置12345678910111213141516171819202122232425262728293031323334353637383940input &#123; beats &#123; port =&gt; 5044 ssl =&gt; true ssl_certificate =&gt; &quot;/etc/pki/tls/certs/logstash-beats.crt&quot; ssl_key =&gt; &quot;/etc/pki/tls/private/logstash-beats.key&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGTIMESTAMP:syslog_timestamp&#125; %&#123;SYSLOGHOST:syslog_hostname&#125; %&#123;DATA:syslog_program&#125;(?:\[%&#123;POSINT:syslog_pid&#125;\])?: %&#123;GREEDYDATA:syslog_message&#125;&quot; &#125; add_field =&gt; [ &quot;received_at&quot;, &quot;%&#123;@timestamp&#125;&quot; ] add_field =&gt; [ &quot;received_from&quot;, &quot;%&#123;host&#125;&quot; ] &#125; syslog_pri &#123; &#125; date &#123; match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ] &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;NGINXACCESS&#125;&quot; &#125; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; #target =&gt; &quot;doc&quot; #remove_field =&gt; [&quot;message&quot;] &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost&quot;] manage_template =&gt; false index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; document_type =&gt; &quot;%&#123;[@metadata][type]&#125;&quot; &#125;&#125; 其他示例配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:## cluster.name: aoemo## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:## node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /usr/share/elasticsearch/data## Path to log files:#path.logs: /var/lib/elasticsearch## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes:## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# Default Kibana 5 file from https://github.com/elastic/kibana/blob/master/config/kibana.yml## Kibana is served by a back end server. This setting specifies the port to use.#server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &apos;localhost&apos;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: &quot;0.0.0.0&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests# to Kibana. This setting cannot end in a slash.#server.basePath: &quot;&quot;# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server&apos;s name. This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URL of the Elasticsearch instance to use for all your queries.#elasticsearch.url: &quot;http://localhost:9200&quot;# When this setting’s value is true Kibana uses the hostname specified in the server.host# setting. When the value of this setting is false, Kibana uses the hostname of the host# that connects to this Kibana instance.#elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations and# dashboards. Kibana creates a new index if the index doesn’t already exist.#kibana.index: &quot;.kibana&quot;# The default application to load.#kibana.defaultAppId: &quot;discover&quot;# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which# is proxied through the Kibana server.elasticsearch.username: &quot;elastic&quot;elasticsearch.password: &quot;changeme&quot;# Paths to the PEM-format SSL certificate and SSL key files, respectively. These# files enable SSL for outgoing requests from the Kibana server to the browser.#server.ssl.cert: /path/to/your/server.crt#server.ssl.key: /path/to/your/server.key# Optional settings that provide the paths to the PEM-format SSL certificate and key files.# These files validate that your Elasticsearch backend uses the same key files.#elasticsearch.ssl.cert: /path/to/your/client.crt#elasticsearch.ssl.key: /path/to/your/client.key# Optional setting that enables you to specify a path to the PEM file for the certificate# authority for your Elasticsearch instance.#elasticsearch.ssl.ca: /path/to/your/CA.pem# To disregard the validity of SSL certificates, change this setting’s value to false.#elasticsearch.ssl.verify: true# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of# the elasticsearch.requestTimeout setting.#elasticsearch.pingTimeout: 1500# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value# must be a positive integer.#elasticsearch.requestTimeout: 30000# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side# headers, set this value to [] (an empty list).#elasticsearch.requestHeadersWhitelist: [ authorization ]# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.#elasticsearch.customHeaders: &#123;&#125;# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.#elasticsearch.shardTimeout: 0# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.#elasticsearch.startupTimeout: 5000# Specifies the path where Kibana creates the process ID file.#pid.file: /var/run/kibana.pid# Enables you specify a file where Kibana stores log output.#logging.dest: stdout# Set the value of this setting to true to suppress all logging output.#logging.silent: false# Set the value of this setting to true to suppress all logging output other than error messages.#logging.quiet: false# Set the value of this setting to true to log all events, including system usage information# and all requests.#logging.verbose: false# Set the interval in milliseconds to sample system and process performance# metrics. Minimum is 100ms. Defaults to 5000.#ops.interval: 5000 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246# Settings file in YAML## Settings can be specified either in hierarchical form, e.g.:## pipeline:# batch:# size: 125# delay: 5## Or as flat keys:## pipeline.batch.size: 125# pipeline.batch.delay: 5## ------------ Node identity ------------## Use a descriptive name for the node:## node.name: test## If omitted the node name will default to the machine&apos;s host name## ------------ Data path ------------------## Which directory should be used by logstash and its plugins# for any persistent needs. Defaults to LOGSTASH_HOME/data## path.data:## ------------ Pipeline Settings --------------## The ID of the pipeline.## pipeline.id: main## Set the number of workers that will, in parallel, execute the filters+outputs# stage of the pipeline.## This defaults to the number of the host&apos;s CPU cores.## pipeline.workers: 2## How many events to retrieve from inputs before sending to filters+workers## pipeline.batch.size: 125## How long to wait in milliseconds while polling for the next event# before dispatching an undersized batch to filters+outputs## pipeline.batch.delay: 50## Force Logstash to exit during shutdown even if there are still inflight# events in memory. By default, logstash will refuse to quit until all# received events have been pushed to the outputs.## WARNING: enabling this can lead to data loss during shutdown## pipeline.unsafe_shutdown: false## ------------ Pipeline Configuration Settings --------------## Where to fetch the pipeline configuration for the main pipeline## path.config:## Pipeline configuration string for the main pipeline## config.string:## At startup, test if the configuration is valid and exit (dry run)## config.test_and_exit: false## Periodically check if the configuration has changed and reload the pipeline# This can also be triggered manually through the SIGHUP signal## config.reload.automatic: false## How often to check if the pipeline configuration has changed (in seconds)## config.reload.interval: 3s## Show fully compiled configuration as debug log message# NOTE: --log.level must be &apos;debug&apos;## config.debug: false## When enabled, process escaped characters such as \n and \&quot; in strings in the# pipeline configuration files.## config.support_escapes: false## ------------ Module Settings ---------------# Define modules here. Modules definitions must be defined as an array.# The simple way to see this is to prepend each `name` with a `-`, and keep# all associated variables under the `name` they are associated with, and# above the next, like this:## modules:# - name: MODULE_NAME# var.PLUGINTYPE1.PLUGINNAME1.KEY1: VALUE# var.PLUGINTYPE1.PLUGINNAME1.KEY2: VALUE# var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE# var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE## Module variable names must be in the format of## var.PLUGIN_TYPE.PLUGIN_NAME.KEY## modules:## ------------ Cloud Settings ---------------# Define Elastic Cloud settings here.# Format of cloud.id is a base64 value e.g. dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy# and it may have an label prefix e.g. staging:dXMtZ...# This will overwrite &apos;var.elasticsearch.hosts&apos; and &apos;var.kibana.host&apos;# cloud.id: &lt;identifier&gt;## Format of cloud.auth is: &lt;user&gt;:&lt;pass&gt;# This is optional# If supplied this will overwrite &apos;var.elasticsearch.username&apos; and &apos;var.elasticsearch.password&apos;# If supplied this will overwrite &apos;var.kibana.username&apos; and &apos;var.kibana.password&apos;# cloud.auth: elastic:&lt;password&gt;## ------------ Queuing Settings --------------## Internal queuing model, &quot;memory&quot; for legacy in-memory based queuing and# &quot;persisted&quot; for disk-based acked queueing. Defaults is memory## queue.type: memory## If using queue.type: persisted, the directory path where the data files will be stored.# Default is path.data/queue## path.queue:## If using queue.type: persisted, the page data files size. The queue data consists of# append-only data files separated into pages. Default is 64mb## queue.page_capacity: 64mb## If using queue.type: persisted, the maximum number of unread events in the queue.# Default is 0 (unlimited)## queue.max_events: 0## If using queue.type: persisted, the total capacity of the queue in number of bytes.# If you would like more unacked events to be buffered in Logstash, you can increase the# capacity using this setting. Please make sure your disk drive has capacity greater than# the size specified here. If both max_bytes and max_events are specified, Logstash will pick# whichever criteria is reached first# Default is 1024mb or 1gb## queue.max_bytes: 1024mb## If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint# Default is 1024, 0 for unlimited## queue.checkpoint.acks: 1024## If using queue.type: persisted, the maximum number of written events before forcing a checkpoint# Default is 1024, 0 for unlimited## queue.checkpoint.writes: 1024## If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page# Default is 1000, 0 for no periodic checkpoint.## queue.checkpoint.interval: 1000## ------------ Dead-Letter Queue Settings --------------# Flag to turn on dead-letter queue.## dead_letter_queue.enable: false# If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries# will be dropped if they would increase the size of the dead letter queue beyond this setting.# Default is 1024mb# dead_letter_queue.max_bytes: 1024mb# If using dead_letter_queue.enable: true, the directory path where the data files will be stored.# Default is path.data/dead_letter_queue## path.dead_letter_queue:## ------------ Metrics Settings --------------## Bind address for the metrics REST endpoint## http.host: &quot;127.0.0.1&quot;## Bind port for the metrics REST endpoint, this option also accept a range# (9600-9700) and logstash will pick up the first available ports.## http.port: 9600-9700## ------------ Debugging Settings --------------## Options for log.level:# * fatal# * error# * warn# * info (default)# * debug# * trace## log.level: info# path.logs:## ------------ Other Settings --------------## Where to find custom plugins# path.plugins: []## ------------ X-Pack Settings (not applicable for OSS build)--------------## X-Pack Monitoring# https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html#xpack.monitoring.enabled: false#xpack.monitoring.elasticsearch.username: logstash_system#xpack.monitoring.elasticsearch.password: password#xpack.monitoring.elasticsearch.url: [&quot;https://es1:9200&quot;, &quot;https://es2:9200&quot;]#xpack.monitoring.elasticsearch.ssl.ca: [ &quot;/path/to/ca.crt&quot; ]#xpack.monitoring.elasticsearch.ssl.truststore.path: path/to/file#xpack.monitoring.elasticsearch.ssl.truststore.password: password#xpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file#xpack.monitoring.elasticsearch.ssl.keystore.password: password#xpack.monitoring.elasticsearch.ssl.verification_mode: certificate#xpack.monitoring.elasticsearch.sniffing: false#xpack.monitoring.collection.interval: 10s#xpack.monitoring.collection.pipeline.details.enabled: true## X-Pack Management# https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html#xpack.management.enabled: false#xpack.management.pipeline.id: [&quot;main&quot;, &quot;apache_logs&quot;]#xpack.management.elasticsearch.username: logstash_admin_user#xpack.management.elasticsearch.password: password#xpack.management.elasticsearch.url: [&quot;https://es1:9200&quot;, &quot;https://es2:9200&quot;]#xpack.management.elasticsearch.ssl.ca: [ &quot;/path/to/ca.crt&quot; ]#xpack.management.elasticsearch.ssl.truststore.path: /path/to/file#xpack.management.elasticsearch.ssl.truststore.password: password#xpack.management.elasticsearch.ssl.keystore.path: /path/to/file#xpack.management.elasticsearch.ssl.keystore.password: password#xpack.management.elasticsearch.sniffing: false#xpack.management.logstash.poll_interval: 5s 12345678910docker run -p 8001:5601 -p 8002:9200 -p 8003:9300 -p 8004:5044 \-v /data/elk/logstash/conf.d:/etc/logstash/conf.d \-v /data/elk/kibana/config/kibana.yml:/opt/kibana/config/kibana.yml \-v /data/elk/logstash/config/logstash.yml:/opt/logstash/config/logstash.yml \-v /data/elk/elasticsearch/config/elasticsearch.yml:/opt/elasticsearch/config/elasticsearch.yml \-v /data/elk/elasticsearch/data:/usr/share/elasticsearch/data \-v /data/elk/elasticsearch/log/:/var/lib/elasticsearch \-v /data/elk/kibana/run_log/:/var/log/kibana/ \-v /data/elk/logstash/run_log/:/var/log/logstash/ \-v /data/elk/elasticsearch/run_log/:/var/log/elasticsearch/ -it --name elk sebp/elk]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>Kibana</tag>
        <tag>ElasticSsearch</tag>
        <tag>LogStash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK: 使用 Docker 搭建 ELK 环境]]></title>
    <url>%2F2018%2F07%2F20%2FELK-%E4%BD%BF%E7%94%A8-Docker-%E6%90%AD%E5%BB%BA-ELK-%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[​ 这里使用 Docker 直接搭建 ELK 环境, 因为 Docker 有第三方提供了 elk 的包, 减少了配置量.当然也可以用 ELK提供的官方 Docker 包或者直接安装, 但是因为工作需要就那么折腾了. Docker 安装 Docker 官方安装教程 https://docs.docker.com/v17.09/engine/installation/linux/docker-ce/centos/#prerequisites 123456789101112131415161718192021# 更新yum update# 删除旧版本避免冲突sudo yum remove docker \ docker-common \ docker-selinux \ docker-engine# 安装依赖sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2# 安装 Docker 的 yum 源sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo# 安装 Dockersudo yum install docker-ce# 开启 Dockersudo systemctl start docker# 开启 Dockerdocker info 这样 Docker 就安装好了, 一般不会遇到什么问题.需要注意的是这是 CentOS 的安装顺序, 其他系统参考官方教程. ELK 安装 ELK Docker 文档 http://elk-docker.readthedocs.io/ 安装 ELK Docker 镜像 12345678docker pull sebp/elkdocker run -p 5001:5601 -p 9200:9200 -p 5044:5044 -p 9300:9300 \ -v /data/elk/conf.d:/etc/logstash/conf.d -it --name elk sebp/elk# 这个包直接 run 就好了, 需要注意的是端口要映射出来, 如果需要修改配置就要映射配置文件.# 配置文件文件夹在文档中能找到, 相关配置也在里面.# 5601 -&gt; Kibana# 5044 -&gt; Logstash# 9200, 9300 -&gt; Elasticsearch 在文档里面提到一个先决条件, 就是 max_map_count 要大于 262144. Linux 设置 max_map_count 1234# 查看设置的 max_map_count 是多少sysctl vm.max_map_count# 设置 max_map_count 大于300000sysctl -w vm.max_map_count=300000 注意, 必须在主机上更改限制; 不能在容器内更改. Docker for Mac 设置 max_map_count在启动容器时设置环境变量 -e MAX_MAP_COUNT=300000如果没有问题就已经装好了, 可以通过 Kibana 和 Elasticsearch:9200 连接查看是否安装完成.]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>Elasticsearch</tag>
        <tag>Logstash</tag>
        <tag>Kibana</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流畅的Python]]></title>
    <url>%2F2018%2F05%2F05%2F%E6%B5%81%E7%95%85%E7%9A%84python%2F</url>
    <content type="text"><![CDATA[流畅的python阅读笔记，流畅的python主要是讲了python的进阶特性。 第一章 Python数据模型 在 Python 中使用双下划线开头，双下划线结尾的方法就是 Python 的特殊方法。Python 会在碰到特殊句法的时候，会使用特殊方法去执行一些对象操作。这些特殊方法能让对象支持这些操作： 迭代， 集合类， 属性访问， 运算符重载， 函数和方法的调用， 对象的创建和销毁， 字符串表示形式和格式化， 管理上下文。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Web开发测试驱动方法 笔记]]></title>
    <url>%2F2018%2F02%2F26%2FPython-Web%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E9%A9%B1%E5%8A%A8%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Python Web开发测试驱动方法的笔记django version: 1.11.7Python version: 3.6.2 第一部分 (第 1~6章 ): 基础知识第 1 章 功能测试协助安装Django测试山羊: “先测试,先测试,先测试”(重要的事说三遍)测试山羊: “没有测试什么也别做”这一章主要讲的是编写一个简单的测试查看Django是否安装正确,没有什么问题. 第 2 章 使用unittest模块扩展功能测试注释书中有一部分说到了注释.注释是有用的, 可以添加上下文,说明代码的目的,但是简单而重复的注释是毫无意义的.而且有一定风险,如果更新了代码后没有修改注释,会误导别人.例如:12# 把wibble的值增加 1wibble += 1 unittest模块 以test_开头的方法都是测试方法 测试方法的名称应该有意义 setUp和tearDown在该测试类的各个测试方法运行前和运行后执行 预期失败,应该预期测试方法的失败和失败方式 12345678910111213141516class XXTest(unittest.TestCase): def setUp(self): &apos;&apos;&apos;测试开始前执行&apos;&apos;&apos; self.browser = webdriver.Chrome() # 设置浏览器驱动 self.browser.implicitly_wait(3) # 隐式等待,在页面加载的时候让Selenium先等待 def tearDown(self): &apos;&apos;&apos;测试结束后执行&apos;&apos;&apos; self.browser.quit() # 设置退出浏览器 def testxxx(self): self.asserIn(&apos;XX&apos;, self.browser.title) # 测试浏览器标题是不是包含XXif __name__ == &apos;main&apos;: # 启动unittest测试程序,禁止抛出ResourceWarning异常 unittest.main(warnings=&apos;ignore&apos;) 第 3 章 使用单元测试测试简单的首页单元测试和功能测试的区别单元测试: 站在程序员的角度从内部测试应用功能测试: 站在用户的角度从外部测试应用 单元测试和功能测试的工作流程 先写功能测试, 从用户的角度描述应用的新功能 功能测试失败后,想办法编写代码让它通过(至少通过当前失败).使用一个或多个单元测试定义希望代码实现的效果,尽量覆盖每一行代码(至少一个) 单元测试失败后,写最少的应用代码让单元测试通过.直到功能测试有进展. 再次运行功能测试,然后根据新的测试结果编写新的单元测试和代码. 单元测试代码123456789101112131415161718192021222324# tests.pyclass HomePageTest(TestCase): def test_root_page_resolve(self): &quot;&quot;&quot;路径解析测试&quot;&quot;&quot; found = resolve(&apos;/&apos;) self.assertEqual(found.func, home_page) def test_home_page_returns_html(self): &quot;&quot;&quot;页面HTML解析&quot;&quot;&quot; request = HttpRequest() response = home_page(request) self.assertTrue(response.content.startswith(b&apos;&lt;html&gt;&apos;)) self.assertIn(b&apos;&lt;title&gt;To-Do lists&lt;/title&gt;&apos;, response.content) self.assertTrue(response.content.endswith(b&apos;&lt;/html&gt;&apos;))# lists/views.pydef home_page(request): return HttpResponse(&apos;&lt;html&gt;&lt;title&gt;To-Do lists&lt;/title&gt;&lt;/html&gt;)# superlists/urls.pyurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^$&apos;, view=views.home_page, name=&apos;home_page&apos;),] 单元测试时的问题在django 1.11.7中resolve的导入是1from django.urls import resolve resolve()中的path路径是在urls的路径加根目录1234found = resolve(&apos;/lists/&apos;)urls:url(r&apos;^lists/&apos;, view=views.home_page, name=&apos;home_page&apos;), 第 4 章 编写这些测试有什么用 测试有点多了?是不是太琐细了?我们并不是大神,也不是大牛,我们没办法避免错误,哪怕是大神可能也会出现简单的错误.TDD可以记录我们编码的进程,让我们不会反复的犯同一个错误.测试可能会很简单,但是有占位作用,当函数变复杂后就可能没那么容易测试了.在学习新框架的时候TDD也能帮助学习,排除错误. 123456def test_home_page_returns_html(self): &quot;&quot;&quot;页面HTML解析&quot;&quot;&quot; request = HttpRequest() response = home_page(request) expected_html = render_to_string(&apos;home.html&apos;) self.assertEqual(response.content.decode(), expected_html) 第 5 章 保存用户输入单元测试规则 不测试常量 assert语句123assertIn(xx, yy) yy中有没有xxassertEqual(xx,yy) xx和yy是不是一样assertTrue(xx) xx结果是不是True find_element_by 语句1234find_element_by_id(xx)find_element_by_tag_name(xx)find_elements_by_tag_name(xx)]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Python</tag>
        <tag>TDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在aws 创建git 仓库]]></title>
    <url>%2F2018%2F02%2F16%2F%E5%9C%A8aws-%E5%88%9B%E5%BB%BAgit-%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[在aws上创建git仓库 在本地创建ssh公钥和私钥1ssh-keygen -t rsa -C &quot;xxx&quot; 在aws创建authortized_keys文件存放刚才生成的公钥。1234cd ~mkdir .ssh &amp;&amp; cd .sshtouch authorized_keysvi authorized_keys 创建git仓库12mkdir 仓库名称 &amp;&amp; cd 仓库名称git init --bare]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 存储过程实例]]></title>
    <url>%2F2018%2F02%2F16%2FMySQL-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[MySQL 存储过程的一些例子,添加了一些注释 1. 存储过程存储过程11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374CREATE PROCEDURE `SP_eco_stat`(IN report_type VARCHAR(100), IN start_datetime DATETIME, IN end_datetime DATETIME) # 存储过程名称（传入参数） COMMENT &apos;计算经济系统数据&apos; # 备注信息BEGIN # 存储过程开始 DECLARE `from` VARCHAR(100); #变量声明，要在前面 DECLARE `detail` VARCHAR(100); DECLARE `country` VARCHAR(100); DECLARE `change` VARCHAR(100); DECLARE `count` VARCHAR(100); DECLARE `user` VARCHAR(100); DECLARE `stop_flag` INT DEFAULT 0; # DEFAULT设置默认值，没有则为null DECLARE `date` DATE; DECLARE _Cursor CURSOR FOR SELECT # 声明光标 XX 光标名 XX a.`from` AS `from`, a.detail AS `detail`, a.country AS `country`, SUM(a.`change`) AS `change`, SUM(1) AS `count`, count(DISTINCT (a.uid)) AS `user` FROM credits a WHERE a.`time` BETWEEN start_datetime AND end_datetime AND a.`change` &gt; 0 GROUP BY a.`from`, a.`change`, a.country; DECLARE CONTINUE HANDLER FOR SQLSTATE &apos;02000&apos; SET stop_flag = 1; # 声明停止时stop_flag=1 SET `date` = CURDATE() + INTERVAL -(1) DAY; # 变量赋值 # 调试开关 SET @__logCallDebug = 1; CALL SP_LogCall(&apos;SP_eco_stat&apos;, # 调用存储过程 CONCAT_WS(&apos;,&apos;, QUOTE(report_type), QUOTE(start_datetime), QUOTE(end_datetime)), &apos;begin...&apos;); OPEN _Cursor; #打开光标 FETCH _Cursor INTO `from`, `detail`, `country`, `change`, `count`, `user`; # 调用光标并赋值 WHILE `stop_flag`&lt;&gt;1 DO # while循环 IF `country` IS NULL # if 判断 THEN SET `country` = &apos;&apos;; END IF; IF `change` IS NULL THEN SET `change` = &apos;&apos;; END IF; IF `detail` IS NULL THEN SET `detail` = &apos;&apos;; END IF; IF `from` IS NULL THEN SET `from` = &apos;&apos;; END IF; IF `count` IS NULL THEN SET `count` = &apos;&apos;; END IF; REPLACE INTO day_amounts(type1, type2, type3, type4, type5, DATE, amount) VALUES (report_type, `from`, `detail`, `country`, &apos;change&apos;, `date`, `change`), (report_type, `from`, `detail`, `country`, &apos;count&apos;, `date`, `count`), (report_type, `from`, `detail`, `country`, &apos;user&apos;, `date`, `user`); # INSERT INTO的强化版，如果有存在相同的主键时对该行进行更新 FETCH _Cursor INTO `from`, `detail`, `country`, `change`, `count`, `user`; END WHILE;CLOSE _Cursor; # 关闭光标CALL SP_LogCallDebug(&apos;end.&apos;);END # 存储过程结束 存储过程21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192CREATE PROCEDURE SP_day_n_retention(IN calcdate_offset INT) COMMENT &apos;日留存计算&apos; BEGIN DECLARE yesterday DATE; # 调试开关 SET @__logCallDebug = 1; CALL SP_LogCall(&apos;SP_day_n_retention&apos;, CONCAT_WS(&apos;,&apos;, calcdate_offset), &apos;begin...&apos;); # day0安装 CALL SP_LogCallDebug(&apos;day0...&apos;); SET yesterday = adddate(date(NOW()), INTERVAL calcdate_offset DAY); # 获取当前【输入数字】天前的时间 date(NOW()) 现在时间 adddate 【DATE_ADD()的同义词】 获取【输入数字】天前的时间 # 安装量(去重) SET @_install_user_count = ( SELECT amount FROM day_amounts WHERE type1 = &apos;user&apos; AND type3 = &apos;new&apos; AND date BETWEEN CONCAT(yesterday, &apos; 00:00:00&apos;) AND CONCAT(yesterday, &apos; 23:59:59&apos;) # BETWEEN 多少和多少之间 CONCAT 连接字符串 ); # 插入或者更新day0 REPLACE INTO day_amounts ( type1, type2, type3, type4, type5, date, amount ) VALUES ( &apos;user&apos;, &apos; &apos;, &apos;retention&apos;, &apos; &apos;, &apos;day0&apos;, yesterday, @_install_user_count ); IF calcdate_offset &lt; 0 # 判断【输入数字】是否小于0 THEN BEGIN # exec执行错误 DECLARE errStr VARCHAR(10240); DROP TABLE IF EXISTS tmp_uuid; # 如果临时表存在则删除 CREATE TEMPORARY TABLE tmp_uuid ( # 创建临时表，只在当前连接有效 uuid VARCHAR(64) NOT NULL, PRIMARY KEY (uuid) ) ENGINE = MYISAM # 数据库存储引擎 和InnoDB相比更注重性能但功能也较少 CHARACTER SET latin1 COLLATE latin1_general_ci; INSERT tmp_uuid SELECT a.uid FROM users_active a WHERE a.act_time BETWEEN CONCAT(yesterday, &apos; 00:00:00&apos;) AND CONCAT(yesterday, &apos; 23:59:59&apos;); SET @days = &apos;1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,35,42,49,56,60,63,70,77,84,90&apos;; WHILE CHAR_LENGTH(@days) &gt; 0 DO # while 循环 SET @dayN = SUBSTRING_INDEX(@days, &apos;,&apos;, 1); # 截取在第一个【,】之前的字符 CALL SP_LogCallDebug(concat(&apos;day&apos;, @dayN, &apos;...&apos;)); # 日期偏移(例如: day1 = 1, day2 = 2, ...) SET @_dayOffset = (SELECT -CAST(@dayN AS SIGNED)); # CAST 获取一个类型的值并产生另一个类型的值 还有CONVERT(value, type) # 在用户表和日志表里找N天前安装的用户的留存量 DROP TEMPORARY TABLE IF EXISTS day_n_retention; SET @_execStr = CONCAT( &apos;REPLACE INTO day_amounts(type1,type2,type3,type4,type5,date,amount) &apos;, &apos;SELECT &quot;user&quot;, &quot;day&quot;, &quot;retention&quot;, &quot; &quot;, &quot;day&apos;, @dayN, &apos;&quot;, &quot;&apos;, adddate(yesterday, @`_dayOffset`), &apos;&quot;, count(DISTINCT(u.uid))&apos;, &apos;FROM users u, tmp_uuid a &apos;, &apos;WHERE u.uid = a.uuid AND &apos;, &apos;u.reg_time BETWEEN &quot;&apos;, adddate(yesterday, @`_dayOffset`), &apos; 00:00:00&quot; AND &quot;&apos;, adddate(yesterday, @`_dayOffset`), &apos; 23:59:59&quot;&apos; ); CALL SP_LogCallDebug(@_execStr); CALL SP_Exec(@_execStr, errStr); SET @days = RIGHT(@days, LENGTH(@days) - LENGTH(@dayN) - 1); END WHILE; END; END IF ; CALL SP_LogCallDebug(&apos;end.&apos;); END;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>存储过程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL证书生成命令]]></title>
    <url>%2F2018%2F02%2F16%2FSSL%E8%AF%81%E4%B9%A6%E7%94%9F%E6%88%90%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[SSL证书的生成 1234567891011# 生成一个RSA密钥$ openssl genrsa -des3 -out 33iq.key 1024# 拷贝一个不需要输入密码的密钥文件$ openssl rsa -in 33iq.key -out 33iq_nopass.key# 生成一个证书请求$ openssl req -new -key 33iq.key -out 33iq.csr# 自己签发证书$ openssl x509 -req -days 365 -in 33iq.csr -signkey 33iq.key -out 33iq.crt]]></content>
      <categories>
        <category>commond</category>
      </categories>
      <tags>
        <tag>SSL</tag>
        <tag>commond</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django HTTPS配置]]></title>
    <url>%2F2018%2F02%2F16%2FDjango-HTTPS%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Django启动HTTPS服务的配置 在settings.py 中 添加以下配置1234SECURE_PROXY_SSL_HEADER = (&apos;HTTP_X_FORWARDED_PROTO&apos;, &apos;https&apos;)SECURE_SSL_REDIRECT = TrueSESSION_COOKIE_SECURE = TrueCSRF_COOKIE_SECURE = True - Django Django启动HTTPS服务的配置 在settings.py 中 添加以下配置1234SECURE_PROXY_SSL_HEADER = (&apos;HTTP_X_FORWARDED_PROTO&apos;, &apos;https&apos;)SECURE_SSL_REDIRECT = TrueSESSION_COOKIE_SECURE = TrueCSRF_COOKIE_SECURE = True]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx-HTTP-配置]]></title>
    <url>%2F2018%2F02%2F16%2FNginx-HTTP-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[HTTP Nginx配置的记录,和一些解析注释 1234567891011121314151617181920212223user LuGH LuGH; # 指定nginx worker进程运行用户以及用户组worker_processes 2; # 指定了nginx要开启的进程数error_log /home/LuGH/logs/nginx_log/error.log crit; # 用来定义全局错误日志文件pid /usr/local/nginx/logs/nginx.pid; # 用来指定进程id的存储文件位置events &#123; # 用来指定nginx的工作模式及连接数上限 worker_connections 1024; # 定义nginx每个进程的最大连接数&#125;http &#123; include mime.types; # 对配置文件所包含的文件设定 default_type application/octet-stream; # 为标准MIME映射未指定任何内容的文件指定默认的mime类型 sendfile on; # 用于开启高效文件传输模式 keepalive_timeout 65; # 指定了客户端与服务器长连接的超时时间 server &#123; listen 80; # 监听端口 server_name localhost; # 服务器名称 location / &#123; proxy_pass http://localhost:9000; # 忽略代理 proxy_redirect default; # 默认重定向 &#125; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx HTTPS 的简单配置]]></title>
    <url>%2F2018%2F02%2F16%2FNginx-HTTPS-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Nginx 配置的记录，和一些解析注释。 12345678910111213141516171819202122232425262728293031323334353637user LuGH LuGH; # 主模块指令，指定nginx worker进程运行用户以及用户组worker_processes 2; # 指定了nginx要开启的进程数error_log /home/LuGH/logs/nginx_log/error.log crit; # 用来定义全局错误日志文件pid /usr/local/nginx/logs/nginx.pid; # 用来指定进程id的存储文件位置events &#123; # 用来指定nginx的工作模式及连接数上限 worker_connections 1024; # 定义nginx每个进程的最大连接数&#125;http &#123; include mime.types; # 对配置文件所包含的文件设定 default_type application/octet-stream; # 为标准MIME映射未指定任何内容的文件指定默认的MIME类型。 sendfile on; # 用于开启高效文件传输模式 keepalive_timeout 65; # 指定了客户端与服务器长连接的超时时间 upstream sserver &#123; # 提供一个简单方法来实现在轮询和客户端IP之间的后端服务器负荷平衡 server 127.0.0.1:9000; # 后端服务器ip &#125; server &#123; listen 443; # 监听端口 server_name local.com; # 服务器名称 ssl on; # 开启ssl ssl_certificate /home/LuGH/key/server.crt; # ssl证书 ssl_certificate_key /home/LuGH/key/server.key; # ssl证书密匙 ssl_session_timeout 5m; # 分配客户端可以重复使用存储在缓存中的会话参数的时间。 ssl_protocols SSLv2 SSLv3 TLSv1; # 指令启用指定的协议 ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2+EXP; # 指令描述允许的密码。密码以OpenSSL支持的格式分配 ssl_prefer_server_ciphers on; # 需要协议SSLv3和TLSv1服务器密码优于客户端的密码 location / &#123; proxy_set_header X-Forwarded-Proto https; # 重新定义或添加字段传递给代理服务器的请求头 只允许https访问 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 识别通过HTTP代理或负载均衡方式连接到Web服务器的客户端最原始的IP地址的HTTP请求头字段是不是https proxy_set_header Host $http_host; # 设置Host proxy_redirect off; # 关闭重定向 proxy_pass http://sserver; # 忽略代理 &#125; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
</search>
